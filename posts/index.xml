<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Digital Forays</title><link>https://kspicer80.github.io/posts/</link><description>Recent content in Posts on Digital Forays</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Tue, 18 Jan 2022 08:19:43 -0600</lastBuildDate><atom:link href="https://kspicer80.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Letting Excel Go and Departmental Assessment Reports</title><link>https://kspicer80.github.io/posts/2022-01-18-letting-excel-go_06/</link><pubDate>Tue, 18 Jan 2022 08:19:43 -0600</pubDate><guid>https://kspicer80.github.io/posts/2022-01-18-letting-excel-go_06/</guid><description>For those that work in disciplines that are much closer to STEM fields than me over here in philosophy and literature, I have no doubt that most of what I want to showcase here will no doubt be the height of banality. As Department Chair, a good deal of the Assessment Reports each year fall to me. Historically, for the past couple of years I have been more than content to simply just keep all of the scores on our assessed outcomes in one gigantic Excel spreadsheet.</description><content type="html"><![CDATA[<p>For those that work in disciplines that are much closer to STEM fields than me over here in philosophy and literature, I have no doubt that most of what I want to showcase here will no doubt be the height of banality. As Department Chair, a good deal of the Assessment Reports each year fall to me. Historically, for the past couple of years I have been more than content to simply just keep all of the scores on our assessed outcomes in one gigantic Excel spreadsheet. This thing certainly kept and held all the numerical data, but it was a bit of a monstrosity—and as the years went by and all the different kinds of courses and kinds of products our students produced to meet all these outcomes increased and multiplied, it became a little unwieldy.</p>
<p><img src="/images/imgforblogposts/post_6/excel_worksheet_clutter.png" alt=""></p>
<p>The file expanded and expanded with all of these tabs, all these extra worksheets, where I tried to keep track of all of these different kinds of things. There is no doubt whatsoever that all of it makes perfect sense to me—I thought I had some sense of where everything was, which cells linked to which other cells on other sheets, so on and so forth. But as time went on I think a lot of these connections became a little frayed to me and I started to wonder if the web here was all that legible. And then the thought of whether or not all these sheets would be legible to someone other than me was icing on the cake. (I kept thinking of those passages from Henri Bergson&rsquo;s <a href="https://brocku.ca/MeadProject/Bergson/Bergson_1911a/Bergson_1911_03.html"><em>Creative Evolution</em></a> where he goes at the age-old philosophical distinction between &ldquo;order&rdquo; and &ldquo;chaos&rdquo; by thinking about the moment when one enters my incredibly &ldquo;messy room&rdquo; and declares it disordered. For me, there&rsquo;s nothing &ldquo;disordered&rdquo; about the room; everything makes perfect sense to me, the clothes are where they are because I threw them there, the bookshelves are stacked three levels deep for all kinds of good reasons—and besides, I know exactly where the Nietzsche section is, I&rsquo;m good with Kierkegaard, and goodness knows I&rsquo;d never forget where the Freud and Lacan are in all this &ldquo;disorder&rdquo; and chaos.)</p>
<p>And then, after doing a bunch of work making my way through Codecademy&rsquo;s <a href="https://www.codecademy.com/learn/paths/data-analyst">data analytics path</a>, I came across Jean-Nicholas Hould&rsquo;s <a href="https://www.jeannicholashould.com/tidy-data-in-python.html">&ldquo;Tidy Data in Python&rdquo;</a> post from a number of years ago now and it was quite a Eureka! moment for me, in so many ways. Again, granted, a Eureka! moment for a total data science neophyte was nothing short of an absolutely inchoate banality for the expert, but none of this lessens the power all this new learning has had for me as of late. Yup, my departmental numbers were in an incredibly &ldquo;messy dataset&rdquo; as <a href="https://vita.had.co.nz/papers/tidy-data.pdf">Hadley Wickham</a> and Hould describe it. So, as the deadline for annual departmental assessment reports rolled around I spent some time tidying up this rather messy dataset. I moved things around with newly-acquired data wrangling skills and got things into a form and structure that followed Wickham&rsquo;s suggestions—as Hould writes:</p>
<blockquote>
<ul>
<li>Each variable forms a column and contains values</li>
<li>Each observation forms a row</li>
<li>Each type of observational unit forms a table</li>
</ul>
</blockquote>
<p>Six years of data—all in one simple CSV file—no more jumping between worksheets in one big file, no more leaping from sheet to sheet to generate graphs or to calculate even the most basic of summary statistics. Again, very far from high-powered machine learning models, K-means clustering, and all of that—just simple data tidying that even the most rudimentary knowledge of the <a href="https://pandas.pydata.org/">pandas library</a> in Python can start to work on quite nicely, quickly, and simply. Nothing fancy, just a nice clean .csv file:</p>
<p><img src="//images/imgforblogposts/post_6/simple_csv_file.png" alt=""></p>
<p>With all that in place it was just a simple hop, skip, and a jump to some simple visualizations of the data:</p>
<p><img src="/images/imgforblogposts/post_6/Figure_1.png" alt=""></p>
<p><img src="/images/imgforblogposts/post_6/Figure_2.png" alt=""></p>
<p><img src="/images/imgforblogposts/post_6/Figure_3.png" alt=""></p>
<p>Again, nothing fancy, but just a whole lot easier to work with everything when it&rsquo;s nice and &ldquo;tidy.&rdquo; Working with and querying the data became so much simpler—a line of code here and there and I could get information about averages—both current and historical—, information on student performance based on instructor, performance on individual outcomes, and so much more. And all because the data was cleaned up. Not to mention that with the structure in place going forward adding to the dataset becomes even simpler as well.</p>
<p>The repo with simple scripts for all this dummy data along with the visualizations is <a href="https://github.com/kspicer80/solo_projects/tree/main/blog_post_projects/departmental_reports">here</a>.</p>
]]></content></item><item><title>Playing with Plain Text and CSS Templates</title><link>https://kspicer80.github.io/posts/2021-12-27-playing-with-plain-text-and-css-templates_05/</link><pubDate>Sun, 26 Dec 2021 12:00:00 -0600</pubDate><guid>https://kspicer80.github.io/posts/2021-12-27-playing-with-plain-text-and-css-templates_05/</guid><description>Over the past few months I have been thinking a great deal about how to incorporate all this new knowledge about digital methodology, data science, data analytics, (scholarly and non-scholarly) work/writing in the &amp;ldquo;digital&amp;rdquo; era into the way I do things as a scholar, teacher, thinker, etc. One of the key people that has been really informative for me has been Jonathan Reeve over at Columbia University. One of his posts from earlier this year, &amp;ldquo;Rethinking the MLA Style Research Paper&amp;rdquo; was incredibly generative for me—and it nicely dovetailed with some work my Departmental colleagues have been doing on thinking about writing at our university (a project generously funded by the Arthur Vining Davis Foundation).</description><content type="html"><![CDATA[<p>Over the past few months I have been thinking a great deal about how to incorporate all this new knowledge about digital methodology, data science, data analytics, (scholarly and non-scholarly) work/writing in the &ldquo;digital&rdquo; era into the way I do things as a scholar, teacher, thinker, etc. One of the key people that has been really informative for me has been <a href="https://jonreeve.com/">Jonathan Reeve</a> over at Columbia University. One of his posts from earlier this year, <a href="https://jonreeve.com/2021/05/rethinking-mla-papers/">&ldquo;Rethinking the MLA Style Research Paper&rdquo;</a> was incredibly generative for me—and it nicely dovetailed with some work my Departmental colleagues have been doing on thinking about writing at our university (a project generously funded by the <a href="https://www.avdf.org/">Arthur Vining Davis Foundation</a>). In addition, I&rsquo;ve been spending a good deal of time with Scott Selisker&rsquo;s wonderful <a href="http://u.arizona.edu/~selisker/post/workflow/">post</a> about the use of plain text writing in academic workflows—I&rsquo;ve learned how to incorporate <a href="https://pandoc.org/">Pandoc</a> and even used Selisker&rsquo;s workflow suggestions to compose my department&rsquo;s assessment report from last year. I think it&rsquo;s just a tad bit frightening to think about how I did things before all this new knowledge. There is an enormous power to working either with plain text or writing everything in <a href="https://www.markdownguide.org/">Markdown</a> and then converting said text into all kinds of different formats via Pandoc. This is not even to mention the ways in which this workflow makes it incredibly easy to incorporate code into the writing—which, of course, prior to very recently, I never had anything to do with—now I feel as if I&rsquo;ve got a vast new area of knowledge to explore and discover. Terribly exciting and exhilarating.</p>
<p>Here in this post what I&rsquo;m hoping to do—admittedly incredibly modest—is combine Reeve&rsquo;s <a href="https://github.com/JonathanReeve/template-research-paper">repository</a> for the aforementioned &ldquo;Rethinking&rdquo; post along with utilizing the .css from <a href="https://edwardtufte.github.io/tufte-css/">Dave Liepmann</a>, which attempts to fashion a style derived from the work of <a href="https://www.edwardtufte.com/tufte/books_vdqi">Edward Tufte</a>, especially the really fantastic stuff found in his <em>The Visual Display of Quantitative Information</em>. We&rsquo;ll see how it goes!</p>
<p>I should also mention a couple of the other places that were helpful:</p>
<ol>
<li>&ldquo;Sustainable Authorship in Plain Text using Pandoc and Markdown&rdquo; by Dennis Tenen and Grant Wythoff over at the <a href="https://programminghistorian.org/en/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown">Programming Historian</a> was a really nice start for me. I haven&rsquo;t gotten a chance to go through Tenen&rsquo;s <em>Plain Text: The Poetics of Composition</em> yet, but I am looking forward to that.</li>
<li>Mort Yao&rsquo;s <a href="https://www.soimort.org/notes/161117/">&ldquo;Boilerplating Pandoc for Academic Writing&rdquo;</a> was also very  useful as I was futzing around with using Pandoc to convert my assessment report from Markdown into .pdf format.</li>
</ol>
]]></content></item><item><title>Visualizing Online Class Discussion Boards</title><link>https://kspicer80.github.io/posts/2019-12-31-visualizing-online-class-discussion-boards_04/</link><pubDate>Tue, 31 Dec 2019 12:00:00 -0600</pubDate><guid>https://kspicer80.github.io/posts/2019-12-31-visualizing-online-class-discussion-boards_04/</guid><description>A Fantastic Anthology
So, this last semester I did another &amp;ldquo;Introduction to Literature&amp;rdquo; course (as I so often do)—this time around I decided to set the theme around that of &amp;ldquo;weird fiction.&amp;rdquo; Since the South Bend DHRI last May I have been thinking a great deal about this whole &amp;ldquo;data visualization&amp;rdquo; thing. I wanted to find some way to visually represent the stories in the anthology that students were gravitating to from week to week.</description><content type="html"><![CDATA[<figure><a href="https://us.macmillan.com/books/9780765333629"><img src="/images/imgforblogposts/post_4/figure_1_the_weird.jpg"
         alt="A Fantastic Anthology"/></a><figcaption>
            <p>A Fantastic Anthology</p>
        </figcaption>
</figure>

<p>So, this last semester I did another &ldquo;Introduction to Literature&rdquo; course (as I so often do)—this time around I decided to set the theme around that of &ldquo;weird fiction.&rdquo; Since the <a href="https://dhsouthbend.org/dhri/">South Bend DHRI</a> last May I have been thinking a great deal about this whole &ldquo;data visualization&rdquo; thing. I wanted to find some way to visually represent the stories in the anthology that students were gravitating to from week to week. I gave minimal guidance—some weeks I would point out that one of my favorite stories was up on the docket or I tried to link the week&rsquo;s discussion to concerns or interests that students had shown in previous weeks. (This was just the old &ldquo;If you liked Julio Cortazar&rsquo;s &ldquo;Axolotl,&rdquo; be sure to read <em>X</em> &hellip;&quot;) There had to be an easy way to represent visually all the student interactions.</p>
<p>Not being an expert, I had to stumble around stupidly and idiotically. Starting with something as silly as a Venn Diagram drawn up in Microsoft Word—hoping to keep track of the kinds of connections students were making to the readings:</p>
<p><img src="/images/imgforblogposts/post_4/figure_2_week_1_discussion_board_venn_diagram.png" alt=""></p>
<p>1st Iteration: just trying to draw the discussion board that week (I have whited-out student names)</p>
<p>Of course, I was first just trying to figure out how to simply draw the connections and not worry so much about the <em>how</em> or the kind of connections being made:</p>
<p><img src="/images/imgforblogposts/post_4/figure_3_week_2_posts_by_story.png" alt=""></p>
<p>2nd Iteration: now simply logging the number of posts about each particular story</p>
<p>Heck—I could draw it by hand, but I wanted to be able to do it not just each week but also be able to produce a graph that would visualize the entire sixteen-week seminar.</p>
<p>And then I came across a tool (IIRC, I found a couple of tutorials on YouTube by <a href="https://www.youtube.com/playlist?list=PLk_jmmkw5S2BqnYBqF2VNPcszY93-ze49">jengolbeck</a>), a tool called <a href="https://gephi.org/"><em>Gephi</em></a> that would allow me to do exactly what I wanted—and quite a bit more, actually. I could take all of the students, myself, and all the texts that we would read, get them into a simple .csv file and then each week I could manually code the connections each individual student was making to the possible texts up for discussion (and also log the times students responded to me or to each other).</p>
<p><img src="/images/imgforblogposts/post_4/figure_4_gephi.png" alt="">
<img src="/images/imgforblogposts/post_4/figure_5_gephi.png" alt="">
<img src="/images/imgforblogposts/post_4/figure_6_gephi.png" alt="">
<img src="/images/imgforblogposts/post_4/figure_7_gephi.png" alt=""></p>
<p>These are just some of the visualizations I played with over the course of the semester (all the green dots are students, the blue dot is me, and all the red ones are stories we read and that students discussed; the size of the nodes are determined by the <a href="https://en.wikipedia.org/wiki/Degree_(graph_theory)">&ldquo;degree&rdquo;</a> of the node). The big red node in the graph in the upper left was the most talked about story out of the entire anthology, <a href="https://en.wikipedia.org/wiki/Leonora_Carrington">Leonora Carrington</a>&rsquo;s 1941 absurdist little tale, &ldquo;White Rabbits.&rdquo;</p>
<p>The tool also gives one some rather basic graph theory and simple statistical information as well. It is easy to have the computer count up the connections, making it rather simple to figure out the most popular texts over the course of the seminar:</p>
<p><img src="/images/imgforblogposts/post_4/figure_10_most_popular.png" alt=""></p>
<p>It was also really simple to see which stories students did not talk about at all (the group in the upper right are all the texts no one mentioned):</p>
<p><img src="/images/imgforblogposts/post_4/figure_9_gephi.png" alt="A Final Graph"></p>
<p>Obviously, there are all kinds of ways to make this data slightly &ldquo;richer.&rdquo; After a week or two I noticed that I did want to get back to the &ldquo;kinds&rdquo; of connections students were making to the texts. Thus, Gephi makes it easy to &ldquo;weight&rdquo; the connections, so I started cataloging <em>how</em> students were interacting: if they cited lines directly from the story, the line was weighted as a 2; if they were strongly referring to the story only through paraphrase, it got a 1.5; no citation whatsoever got a 1. This would then make it easier for me to see how often students were doing something as basic as directly citing the story under discussion in their posts.</p>
<p>It would be cool—I&rsquo;m not sure at all if there is a way to do this from within Gephi—if one could time lapse this whole thing so one could watch the connections get drawn over the course of the semester. One can—and I did—keep track of which connections got made each week (that&rsquo;s as simple as adding another column in the .csv): I had originally wanted to see if I could track the size of my own node over time: my plan was to be a really frequent participant in the first few weeks of the semester. It&rsquo;s easy to show in the graph how those first few weeks of the semester saw me responding to every single student post. I had hoped that my node would become slightly smaller and smaller as the semester went on. By and large, that was true.</p>
<p>I have no doubt that I will continue to fiddle around with all over this the next time I teach this course (which will be in the Spring here after the start of the new year). What kinds of things will I see popping up when I do all this a second time? Will I see a similarity in terms of popularity of texts over the semester? Will I see that similar phenomenon where we have a somewhat small(ish) group of students who are really &ldquo;talkative&rdquo; as we usually do in our f2f classrooms (while the larger group is slightly more reserved)? I have no clue—I&rsquo;m curious to see. (I spent a little bit of time, not much, dabbling with tools/libraries in Python that would do something similar—I know there is <a href="https://igraph.org/python/">iGraph</a> and there no doubt have to be others that I don&rsquo;t even know about. Another slightly more &ldquo;technical&rdquo; rather than philosophical question would be whether or not I would want to figure out a way to automate all of this rather than coding it by hand, so to speak. I know that spaCy has some really powerful tools that one could use to do this: create a custom NER [Named Entity Recognition] with the names of the texts, students, etc., write a script to have the computer find which students are looking at which stories, and then to have it draw the network graph. It would also be somewhat simple enough to think of a way to script the whole process that would also allow you to set some criterion for the whole &ldquo;weighting&rdquo; of the edges, too, though that would need to be a little bit more sophisticated.)</p>
<p>More to come, as always &hellip;</p>
]]></content></item><item><title>Still Playing Around with Python, NLTK, spaCy, etc.</title><link>https://kspicer80.github.io/posts/2019-12-24-still-playing-around-with-python-nltk-spacy-etc_03/</link><pubDate>Wed, 07 Aug 2019 12:05:00 -0600</pubDate><guid>https://kspicer80.github.io/posts/2019-12-24-still-playing-around-with-python-nltk-spacy-etc_03/</guid><description>Ugh, so clearly my discipline in posting is nonexistent. It&amp;rsquo;s fine—I&amp;rsquo;m still just dipping my toes in the water here—and not only that, I&amp;rsquo;m interested in trying to document my learning curve here, such as it is &amp;hellip; here&amp;rsquo;s a WordCloud for Bram Stoker&amp;rsquo;s Dracula:
This past July I got an opportunity to do a NEH Summer Seminar over at the University of Iowa (&amp;quot;Religion, Secularism, and the Novel&amp;quot;), where the group read a number of canonical novels (Crusoe, Silas Marner, Dracula, On the Road, and Home).</description><content type="html"><![CDATA[<p>Ugh, so clearly my discipline in posting is nonexistent. It&rsquo;s fine—I&rsquo;m still just dipping my toes in the water here—and not only that, I&rsquo;m interested in trying to document my learning curve here, such as it is &hellip; here&rsquo;s a WordCloud for Bram Stoker&rsquo;s <em>Dracula</em>:</p>
<p><img src="/images/imgforblogposts/post_3/figure_1_stoker_word_cloud.png" alt="Fig. 1: Word Cloud for Bram Stoker&rsquo;s Dracula"></p>
<p>This past July I got an opportunity to do a NEH Summer Seminar over at the University of Iowa (&quot;<a href="https://religion-secularism-novel.sites.uiowa.edu/">Religion, Secularism, and the Novel</a>&quot;), where the group read a number of canonical novels (<em>Crusoe</em>, <em>Silas Marner</em>, <em>Dracula</em>, <em>On the Road</em>, and <em>Home</em>). I enjoyed the seminar and loved all the so-called traditional humanities stuff: deep discussion, attentiveness to rhetoric and form, and all the other things that we literary scholars do so well. I did also fidget around with some of the newer DH methods (at the time it was really just basic computational stuff: counting tokens, plotting frequency dispersions, making silly little Word Clouds, etc.) on those very same texts we were looking at (excluding Robinson&rsquo;s <em>Home</em>).</p>
<p>Here&rsquo;s a <a href="https://www.nltk.org/book_1ed/ch01.html">&ldquo;lexical dispersion plot&rdquo;</a> for the word &ldquo;time&rdquo; in Stoker&rsquo;s <em>Dracula</em>:</p>
<p><img src="/images/imgforblogposts/post_3/figure_2_time_in_stoker.png" alt="Time"></p>
<p>Here&rsquo;s a dispersion plot for some of the major characters in <em>Dracula</em>:</p>
<p><img src="/images/imgforblogposts/post_3/figure_3_character_dispersion_plots.png" alt="Character Dispersion Plots"></p>
<p>Here are some common nouns in <em>Dracula</em>:</p>
<p><img src="/images/imgforblogposts/post_3/figure_4_common_word_counts.png" alt="Common Nouns"></p>
<p>Another Dispersion Plot</p>
<p>I didn&rsquo;t really find anything too profound—although it did for some reason surprise me just slightly that a word like &ldquo;time&rdquo; occurred as frequently as it did in <em>Dracula</em> (simple counts found it 373 times [full exploratory notebook for this is available <a href="https://github.com/kspicer80/nehsummerseminar2019playground/blob/master/nehexploratorynotebook.ipynb">here</a>—the dispersion plots too at this moment in time aren&rsquo;t actually plotting in Jupyter Notebooks {the <a href="https://github.com/googlecolab/colabtools/issues/397">issue</a>, it seems, is <a href="https://stackoverflow.com/questions/54264548/nltk-lexical-dispersion-plot-does-not-show-on-google-colab">known</a> as of 24 December 2019}]).</p>
<p>I also spent a little bit of time messing around with <a href="https://spacy.io/">spaCy</a>—tinkering around with the &ldquo;Part of Speech&rdquo; (POS) tagger and other things on the four texts. Here are some counts of different parts of speech in the four novels:</p>
<p><img src="/images/imgforblogposts/post_3/figure_5_kinds_of_words_kerouac.png" alt="Different Counts of Parts Speech"></p>
<p>Dean, in <em>On the Road</em>, always trying to live in the eternal now, makes total sense the verb &ldquo;to be&rdquo; would show up all around him.</p>
<p><img src="/images/imgforblogposts/post_3/figure_6_kerouac_gender.png" alt="Dean Verbs"></p>
<p>The code to generate the above figure is available <a href="https://gist.github.com/kspicer80/f78d0cfccc43a9e07b05efca6b652b96">here</a>)</p>
<p>One of the day&rsquo;s questions about <em>On the Road</em> at the NEH Seminar had to do with what exactly it is like to read a book like <em>On the Road</em> after the #MeToo movement. Some quick counting of the verbs used to describe Marylou, for instance, comes up as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"> (<span style="color:#e6db74">&#39;be&#39;</span>, <span style="color:#ae81ff">34</span>),
 (<span style="color:#e6db74">&#39;have&#39;</span>, <span style="color:#ae81ff">10</span>),
 (<span style="color:#e6db74">&#39;want&#39;</span>, <span style="color:#ae81ff">10</span>),
 (<span style="color:#e6db74">&#39;know&#39;</span>, <span style="color:#ae81ff">9</span>),
 (<span style="color:#e6db74">&#39;go&#39;</span>, <span style="color:#ae81ff">9</span>),
 (<span style="color:#e6db74">&#39;say&#39;</span>, <span style="color:#ae81ff">8</span>),
 (<span style="color:#e6db74">&#39;sleep&#39;</span>, <span style="color:#ae81ff">7</span>),
 (<span style="color:#e6db74">&#39;see&#39;</span>, <span style="color:#ae81ff">6</span>),
 (<span style="color:#e6db74">&#39;get&#39;</span>, <span style="color:#ae81ff">5</span>),
 (<span style="color:#e6db74">&#39;sit&#39;</span>, <span style="color:#ae81ff">5</span>),
 (<span style="color:#e6db74">&#39;make&#39;</span>, <span style="color:#ae81ff">5</span>),
 (<span style="color:#e6db74">&#39;take&#39;</span>, <span style="color:#ae81ff">5</span>),
 (<span style="color:#e6db74">&#39;tell&#39;</span>, <span style="color:#ae81ff">4</span>),
 (<span style="color:#e6db74">&#39;find&#39;</span>, <span style="color:#ae81ff">4</span>),
 (<span style="color:#e6db74">&#39;jump&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;run&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;do&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;drive&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;wait&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;lean&#39;</span>, <span style="color:#ae81ff">3</span>)
</code></pre></div><p>Looking at Sal:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"> (<span style="color:#e6db74">&#39;be&#39;</span>, <span style="color:#ae81ff">25</span>),
 (<span style="color:#e6db74">&#39;go&#39;</span>, <span style="color:#ae81ff">18</span>),
 (<span style="color:#e6db74">&#39;say&#39;</span>, <span style="color:#ae81ff">14</span>),
 (<span style="color:#e6db74">&#39;get&#39;</span>, <span style="color:#ae81ff">10</span>),
 (<span style="color:#e6db74">&#39;tell&#39;</span>, <span style="color:#ae81ff">8</span>),
 (<span style="color:#e6db74">&#39;think&#39;</span>, <span style="color:#ae81ff">8</span>),
 (<span style="color:#e6db74">&#39;have&#39;</span>, <span style="color:#ae81ff">7</span>),
 (<span style="color:#e6db74">&#39;want&#39;</span>, <span style="color:#ae81ff">5</span>),
 (<span style="color:#e6db74">&#39;know&#39;</span>, <span style="color:#ae81ff">4</span>),
 (<span style="color:#e6db74">&#39;come&#39;</span>, <span style="color:#ae81ff">4</span>),
 (<span style="color:#e6db74">&#39;dig&#39;</span>, <span style="color:#ae81ff">4</span>),
 (<span style="color:#e6db74">&#39;do&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;call&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;let&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;make&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;see&#39;</span>, <span style="color:#ae81ff">3</span>),
 (<span style="color:#e6db74">&#39;ask&#39;</span>, <span style="color:#ae81ff">2</span>),
 (<span style="color:#e6db74">&#39;arrive&#39;</span>, <span style="color:#ae81ff">2</span>),
 (<span style="color:#e6db74">&#39;remember&#39;</span>, <span style="color:#ae81ff">2</span>),
 (<span style="color:#e6db74">&#39;find&#39;</span>, <span style="color:#ae81ff">2</span>)
</code></pre></div><p>And now at Dean:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"> (<span style="color:#e6db74">&#39;be&#39;</span>, <span style="color:#ae81ff">198</span>),
 (<span style="color:#e6db74">&#39;say&#39;</span>, <span style="color:#ae81ff">105</span>),
 (<span style="color:#e6db74">&#39;go&#39;</span>, <span style="color:#ae81ff">44</span>),
 (<span style="color:#e6db74">&#39;have&#39;</span>, <span style="color:#ae81ff">41</span>),
 (<span style="color:#e6db74">&#39;take&#39;</span>, <span style="color:#ae81ff">29</span>),
 (<span style="color:#e6db74">&#39;come&#39;</span>, <span style="color:#ae81ff">28</span>),
 (<span style="color:#e6db74">&#39;see&#39;</span>, <span style="color:#ae81ff">27</span>),
 (<span style="color:#e6db74">&#39;know&#39;</span>, <span style="color:#ae81ff">26</span>),
 (<span style="color:#e6db74">&#39;tell&#39;</span>, <span style="color:#ae81ff">25</span>),
 (<span style="color:#e6db74">&#39;get&#39;</span>, <span style="color:#ae81ff">25</span>),
 (<span style="color:#e6db74">&#39;yell&#39;</span>, <span style="color:#ae81ff">21</span>),
 (<span style="color:#e6db74">&#39;do&#39;</span>, <span style="color:#ae81ff">17</span>),
 (<span style="color:#e6db74">&#39;drive&#39;</span>, <span style="color:#ae81ff">17</span>),
 (<span style="color:#e6db74">&#39;want&#39;</span>, <span style="color:#ae81ff">14</span>),
 (<span style="color:#e6db74">&#39;cry&#39;</span>, <span style="color:#ae81ff">14</span>),
 (<span style="color:#e6db74">&#39;sit&#39;</span>, <span style="color:#ae81ff">13</span>),
 (<span style="color:#e6db74">&#39;look&#39;</span>, <span style="color:#ae81ff">12</span>),
 (<span style="color:#e6db74">&#39;talk&#39;</span>, <span style="color:#ae81ff">11</span>),
 (<span style="color:#e6db74">&#39;sleep&#39;</span>, <span style="color:#ae81ff">11</span>),
 (<span style="color:#e6db74">&#39;stand&#39;</span>, <span style="color:#ae81ff">11</span>)
</code></pre></div><p>As I say, I&rsquo;m still just dipping my toes in &hellip; more to come I&rsquo;m sure.</p>
<p>P.S. Code used to generate things here can be found in <a href="https://github.com/kspicer80/nehsummerseminar2019playground">this repo</a>.</p>
]]></content></item><item><title>More (Initial) Digital Humanities Stuff</title><link>https://kspicer80.github.io/posts/2019-07-06-more-initial-digital-humanities-stuff_02/</link><pubDate>Sat, 06 Jul 2019 12:05:00 -0600</pubDate><guid>https://kspicer80.github.io/posts/2019-07-06-more-initial-digital-humanities-stuff_02/</guid><description>I am totally not under the impression that any of these initial thoughts on my inchoate exploration of the DH field (a field whose history I am only now beginning to really grasp) will be of any use whatsoever. I am just beginning to dip my toes into water that others are already expert swimmers in—and have been for many many years. I am not sure either if the kinds of things these digital or computational tools can do will be all they helpful to me in the long run.</description><content type="html"><![CDATA[<p>I am totally not under the impression that any of these initial thoughts on my inchoate exploration of the DH field (a field whose history I am only now beginning to really grasp) will be of any use whatsoever. I am just beginning to dip my toes into water that others are already expert swimmers in—and have been for many many years. </p>
<p>I am not sure either if the kinds of things these digital or computational tools can do will be all they helpful to me in the long run.  The literature is rife with the central question that no doubt is preeminent for most scholars in the humanities, especially those of us who cut our teeth on &ldquo;close reading.&rdquo;  At the same time, learning about some of these new tools would seem to be fine give my recent (as of around July or August of last year) turn to a much more Deleuzian—rather than Derridean—philosophical posture and style of thinking.  Why not use these tools to proliferate new and bizarre ways of thinking about what it is humanities scholars do?  Why not take Nabokov&rsquo;s <em>Lolita</em> and feed it through a bunch of algorithms designed for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>? (Actually, I&rsquo;m in the process of doing just that and it strikes me that Nabokov&rsquo;s novel would seem to be one of those that should be a nice testing ground for all kinds of work in DH [and it could be already for all I know; as I say, I&rsquo;m just barely getting a sense for what&rsquo;s been done in the field so far, it&rsquo;s all totally brand new to me).  Even from the very first line of that novel Nabokov is playing with the very stuff of language—carving up into phonemes the name of the eponymous love interest of Humbert Humbert.  I also noticed, just flipping through my copy recently, of what a morass <em>Lolita</em> could be for someone first training their (or their algorithm&rsquo;s sights on it):  the constant switch from English to French, the never-ending insertions of French phrases and names and dialogue, and—just staying within the language of English:   Nabokov&rsquo;s constant attention to and awareness of the phonetic stuff of language, e.g. take this from Part One:</p>
<blockquote>
<p>&hellip; I kept repeating this automatic stuff and holding her under its special spell (spell because of the garbling), and all the while I was mortally afraid that some act of God might interrupt me, might remove the golden load in the sensation of which all my being seemed concentrated, and this anxiety forced me to work, for the first minute or so, more hastily than was consensual with deliberately modulated enjoyment.  <em>The stars that sparkled, and the cars that parkled, and the bars, and the barmen were presently taken over by her &hellip;</em> (<em>The Annotated Lolita</em>, ed. Alfred Appel, Jr. [New York:  McGraw-Hill Book Co., 1970], p. 61)</p>
</blockquote>
<p>We all know what a distinguished polymath Nabokov was—and I wonder what DH methods of textual analysis and text mining such a text would be amenable to &hellip; As I say, I&rsquo;ll get a cleaned-up version of the text ready and give it a go.  (I should also probably give a shout out to David H— whose essay on Humbert&rsquo;s aestheticism had been bouncing around in my head all week while over at Notre Dame; not sure why that was &hellip;)  Why do I always feel this urge to never close a parenthesis like this—or at the very least to keep proliferating it indefinitely &hellip;?)  What would a sentiment analysis algorithm do with all of H.P. Lovecraft&rsquo;s fiction?  Anything useful? (I&rsquo;ll get on this one too and report back &hellip;)  </p>
<p>As I say, it&rsquo;s possible all these new tools are totally unhelpful, although I suspect that&rsquo;s not going to end up being the case.  Just one example to suffice for now.  The South Bend DHRI sent along some reading materials to all the participants ahead of the institute.  One little remark in Dennis Tenen&rsquo;s article in <em>Debates in the Digital Humanities</em>, <a href="http://dhdebates.gc.cuny.edu/debates/text/60">&ldquo;Blunt Instrumentalism: On Tools and Methods&rdquo;</a> mentions the k-means clustering method in topic modeling as an algorithm that is non-deterministic, which means that it &ldquo;will perform differently each time it is run &hellip;&rdquo;  Last academic year while working on the English Department&rsquo;s Program Review I spent a great of time thinking about what it our faculty mean when they talk about &ldquo;close reading.&rdquo;  I found myself returning again and again to talking about the ontology and philosophy of events, as that word is used (very differently) by Derrida and Deleuze. I found myself again and again running into this paradoxical situation where we describe the coming together of reader, text, and this third thing, the &ldquo;event of reading,&rdquo; which is absolutely singular and never to be repeated; simultaneously, this &ldquo;event of reading&rdquo; must be iterable, to use Derrida&rsquo;s language.  I think this idiom of non-deterministic algorithmic behavior could be helpful in giving me another way to describe what it is that we do when we closely read something.  We do something algorithmic—we do not read arbitrarily, simply following our own whims and caprices—; there is a method to our madness, but it is not a deterministic one.</p>
<p>(Rereading some parts of Rita Felski&rsquo;s <em>The Limits of Critique</em> for the <a href="https://religion-secularism-novel.sites.uiowa.edu/">NEH Summer Seminar</a> I&rsquo;ll be attending here very soon, I came across the following:</p>
<blockquote>
<p>One indispensable link in this chain of mediators is a reader whose response is never entirely predictable or knowable.  It is here that literary studies need to steer clear of a vulgar sociology (where a reader is reduced to the sum of her demographical data) as well as of a one-dimensional theory of language (where a reader is a nodal point through which language or discourse flows). Readers are not autonomous, self-contained, centers of meaning, but they are also not mere flotsam and jetsam tossed on the tides of social or linguistic forces that they are helpless to affect or comprehend. When they encounter texts, they do so in all their commonality and quirkiness; they mediate and in turn are mediated, in both predictable and perplexing ways. (<em>The Limits of Critique</em> [Chicago:  U of Chicago P, 2015], p. 171.)</p>
</blockquote>
<p>This is very nicely put—I dig it.)</p>
<figure><a href="https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf"><img src="/images/imgforblogposts/post_2/moretti.jpg"
         alt="Fig. 1: Original Image from one of Franco Moretti&amp;rsquo;s Lit Lab papers, cited in Karen Schulz&amp;rsquo;s 2011 NYT article, &amp;ldquo;The Mechanic Muse&amp;rdquo;"/></a><figcaption>
            <p>Fig. 1: Original Image from one of Franco Moretti&rsquo;s <em>Lit Lab</em> papers, cited in Karen Schulz&rsquo;s 2011 <em>NYT</em> <a href="https://www.nytimes.com/2011/06/26/books/review/the-mechanic-muse-what-is-distant-reading.html">article</a>, &ldquo;The Mechanic Muse&rdquo;</p>
        </figcaption>
</figure>

<p>The whole &ldquo;close vs. distant&rdquo; reading issue is no doubt just about gatekeeping, as Ted Underwood has <a href="http://dhdebates.gc.cuny.edu/debates/text/95">argued</a>, and is one of those things where my (re)turn to Deleuze is helpful:  a logic of opposition should not be chosen over a language of difference—distant reading is perhaps just simply a different way of reading and not necessarily opposed to reading closely. An unsupervised (or supervised) nondeterministic algorithm is what we do when we humans read closely?  Why not?</p>
<p>P.S. Next on my reading list is Wendy Nelson Espeland and Mitchell L. Steven&rsquo;s 2008 <a href="https://www.jstor.org/stable/23998802?seq=1#page_scan_tab_contents">essay</a>, &ldquo;A Sociology of Quantification&rdquo; and Katherine Bode&rsquo;s <a href="https://read.dukeupress.edu/modern-language-quarterly/article-abstract/78/1/77/19924/The-Equivalence-of-Close-and-Distant-Reading-or">essay</a> in <em>MLQ</em>, &ldquo;The Equivalence of &lsquo;Close&rsquo; and &lsquo;Distant&rsquo; Reading; or, Toward a New Object for Data-Rich Literary History.&rdquo;</p>
]]></content></item><item><title>Inaugural Post and ... Re-posting an Old Post (1 of 4) from an Older WordPress site</title><link>https://kspicer80.github.io/posts/2019-07-06-inaugural-post_01/</link><pubDate>Sat, 06 Jul 2019 12:00:00 -0600</pubDate><guid>https://kspicer80.github.io/posts/2019-07-06-inaugural-post_01/</guid><description>Okay, so here goes for an inaugural post—hoping to essentially combine my graduate work (in Continental Philosophy and Derridean deconstruction—where the X without X [updated 12.24.2021: the old website this initial post came from was at xwithoutxdrs.wordpress.com] locution is so at home in the work of Blanchot, Derrida, et. al.)—with all of the new toys I am learning to play with from the Digital Humanities.
So after a week-long research institute put together by some fantastic people at the College of St.</description><content type="html"><![CDATA[<p>Okay, so here goes for an inaugural post—hoping to essentially combine my graduate work (in Continental Philosophy and Derridean deconstruction—where the <em>X without X</em> [updated 12.24.2021: the old website this initial post came from was at xwithoutxdrs.wordpress.com] locution is so at home in the work of Blanchot, Derrida, et. al.)—with all of the new toys I am learning to play with from the Digital Humanities.</p>
<p>So after a week-long <a href="http://dhsouthbend.org/dhri/">research institute</a> put together by some fantastic people at the College of St. Mary&rsquo;s and Notre Dame earlier in the summer, I got my hands dirty with a whole host of tools of the trade within the Digital Humanities.  As one can see from the <a href="http://dhsouthbend.org/dhri/curriculum/">curriculum</a>, the week was rather intense and both I and <a href="https://annaioanes.wordpress.com/">my colleague</a> came back exhausted, to be sure, but also full of all kinds of new knowledge and (best of all) a sense of how much knowledge we still need to try to acquire.</p>
<p>Playing around with <a href="https://spacy.io/">spaCy</a>-displaCy and a sentence from Beverly Cleary&rsquo;s <em>Ramona Forever</em> (image shows a &ldquo;dependency parsing&rdquo; for the novel&rsquo;s first sentence—my youngest child is reading all of these books at the moment):</p>
<p><img src="/images/imgforblogposts/post_1/ramona_sentence.svg" alt=""></p>
<p>Feel free to go and compare the nicer (less black-and-white version) provided by Google&rsquo;s Cloud Natural Language API <a href="https://cloud.google.com/natural-language/">here</a>. </p>
<p>More to come &hellip; (I&rsquo;m also hoping to use this whole site as a kind of sandbox in which to play around with many of these new toys, not to mention just Wordpress itself &hellip; [I need a bunch more time playing around with this new &ldquo;block&rdquo; editor thing here too[updated as of 12.24.2021: I need more time to fiddle around with Markdown, HTML, and a bunch of other stuff]]—nobody&rsquo;ll read these things anyways, so here seems as good a place as any, lol &hellip;)</p>
<p>The simple code to generate the displaCy figure is here:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> spacy
nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;en_core_web_lg&#34;</span>)
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Guess what? Ramona Quimby asked one Friday evening when her Aunt Beatrice dropped by to show off her new ski clothes and to stay for supper.&#34;</span>

doc <span style="color:#f92672">=</span> nlp(sentence)
dep_image <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>displacy<span style="color:#f92672">.</span>render(doc, style<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dep&#34;</span>, minify<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
output_path <span style="color:#f92672">=</span> Path(<span style="color:#e6db74">&#34;ramona_sentence.svg&#34;</span>)
output_path<span style="color:#f92672">.</span>open(<span style="color:#e6db74">&#34;w&#34;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;utf-8&#34;</span>)<span style="color:#f92672">.</span>write(dep_image)
</code></pre></div>]]></content></item></channel></rss>