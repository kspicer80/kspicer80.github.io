<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="In my DH journey, the work of Dr. William Mattingly has been something of a constant companion and guide.
His video laying out the steps for a &amp;ldquo;Binary Data Classification&amp;rdquo; was clear and quite clarifying for me in all kinds of ways. The classifier&amp;rsquo;s job in this video was to see if it could tell the difference between the work of Oscar Wilde and Dan Brown. What if we tried a different pair of authors/texts?"><meta name=keywords content=",machine learning,willa cather,sarah orne jewett,digital humanities,python for the digital humanities,Python,k-nearest neighbors,keras,tensorflow,literary style,word frequency,word frequencies,sklearn"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://kspicer80.github.io/posts/2022-04-10-classification-fun-with-some-literary-texts/><title>Binary Text Classification Fun! ... with some Literary Texts by Willa Cather and Sarah Orne Jewett :: Digital Forays — Tracing Paths through the Digital Humanities</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.4e5c639214707eff609bb55fe49e183dee42258a73bc90e4cc7b0a84f900798a.css><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content><meta itemprop=name content="Binary Text Classification Fun! ... with some Literary Texts by Willa Cather and Sarah Orne Jewett"><meta itemprop=description content="In my DH journey, the work of Dr. William Mattingly has been something of a constant companion and guide.
His video laying out the steps for a &ldquo;Binary Data Classification&rdquo; was clear and quite clarifying for me in all kinds of ways. The classifier&rsquo;s job in this video was to see if it could tell the difference between the work of Oscar Wilde and Dan Brown. What if we tried a different pair of authors/texts?"><meta itemprop=datePublished content="2022-04-01T00:00:00+00:00"><meta itemprop=dateModified content="2022-04-01T00:00:00+00:00"><meta itemprop=wordCount content="1069"><meta itemprop=image content="https://kspicer80.github.io/"><meta itemprop=keywords content="machine learning,willa cather,sarah orne jewett,digital humanities,python for the digital humanities,Python,k-nearest neighbors,keras,tensorflow,literary style,word frequency,word frequencies,sklearn,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kspicer80.github.io/"><meta name=twitter:title content="Binary Text Classification Fun! ... with some Literary Texts by Willa Cather and Sarah Orne Jewett"><meta name=twitter:description content="In my DH journey, the work of Dr. William Mattingly has been something of a constant companion and guide.
His video laying out the steps for a &ldquo;Binary Data Classification&rdquo; was clear and quite clarifying for me in all kinds of ways. The classifier&rsquo;s job in this video was to see if it could tell the difference between the work of Oscar Wilde and Dan Brown. What if we tried a different pair of authors/texts?"><meta property="og:title" content="Binary Text Classification Fun! ... with some Literary Texts by Willa Cather and Sarah Orne Jewett"><meta property="og:description" content="In my DH journey, the work of Dr. William Mattingly has been something of a constant companion and guide.
His video laying out the steps for a &ldquo;Binary Data Classification&rdquo; was clear and quite clarifying for me in all kinds of ways. The classifier&rsquo;s job in this video was to see if it could tell the difference between the work of Oscar Wilde and Dan Brown. What if we tried a different pair of authors/texts?"><meta property="og:type" content="article"><meta property="og:url" content="https://kspicer80.github.io/posts/2022-04-10-classification-fun-with-some-literary-texts/"><meta property="og:image" content="https://kspicer80.github.io/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-01T00:00:00+00:00"><meta property="article:modified_time" content="2022-04-01T00:00:00+00:00"><meta property="og:site_name" content="Digital Forays"><meta property="article:published_time" content="2022-04-01 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>conda activate</span>
<span class=logo__cursor></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>Blog</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>6 minutes</p></div><article><h1 class=post-title><a href=https://kspicer80.github.io/posts/2022-04-10-classification-fun-with-some-literary-texts/>Binary Text Classification Fun! &mldr; with some Literary Texts by Willa Cather and Sarah Orne Jewett</a></h1><div class=post-content><p>In my DH journey, the work of <a href=https://pythonhumanities.com/python-for-dh-course/>Dr. William Mattingly</a> has been something of a constant companion and guide.</p><p>His video laying out the steps for a <a href="https://www.youtube.com/watch?v=tPgQH5UTC9k">&ldquo;Binary Data Classification&rdquo;</a> was clear and quite clarifying for me in all kinds of ways. The classifier&rsquo;s job in this video was to see if it could tell the difference between the work of Oscar Wilde and Dan Brown. What if we tried a different pair of authors/texts? As I offhandedly mentioned in my previous <a href=https://kspicer80.github.io/posts/2022-03-29-vector-space-models-and-shakespeare/>post</a>, finding texts in the public domain (like Shakespeare&rsquo;s for example), saves one quite a bit of time wrangling things together. I also, for some, reason, have found myself going back to some of the works of Willa Cather (admittedly, she&rsquo;s definitely not in my wheelhouse/field of study by any stretch of the imagination) and was wondering how a machine learning classifier might perform when looking at Cather&rsquo;s texts alongside those of one of her key influences, Sarah Orne Jewett (image below is courtesy of UNL&rsquo;s fantastic <em>Willa Cather Archive</em> <a href=https://cather.unl.edu/writings/books/0017>here</a>):</p><p><img src=/images/imgforblogposts/post_10/o_pioneers_1913_dedication_to_jewett.png alt="Cather Dedication to Jewett 1913"></p><p>Why not?—I figured I could tinker around a bit to see what&rsquo;s possible. Project Gutenberg has a number of texts by both <a href=https://www.gutenberg.org/ebooks/author/22>Cather</a> and <a href=https://www.gutenberg.org/ebooks/author/202>Jewett</a>—and the UNL <em>Willa Cather Archive</em> has a good number of her texts <a href=https://cather.unl.edu/writings/books>formatted in .xml</a>. I grabbed as many of the texts from these two sources. Since the Gutenberg texts come with the typical boilerplate material at the start and end of the plain text file I pulled out a nice little script from <a href=https://github.com/c-w/gutenberg>&ldquo;C-W&rdquo; on GitHub</a> that had a nice list of phrases used in the boilerplate material; <a href=https://github.com/kspicer80/authorship_attribution_studies/blob/main/cather_jewett/strip_headers_and_footers.py>the simple script</a> would strip all the boilerplate when the text files get read in.</p><p>The major steps here for this tinkering is to read in all the plain text files, split everything up by sentences, write a function that will &ldquo;pad&rdquo; the data so that any sentences that are less than a specific length ultimately get a <code>?</code> added to the sentence to bring it&rsquo;s length up to the already specified <code>max_length</code> of each sentence. This is due to the way that the keras library likes to have data fed to it. That function looks as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>padding_data</span>(sentences, index, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>25</span>):
</span></span><span style=display:flex><span>    new_sentences <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> sentence <span style=color:#f92672>in</span> sentences:
</span></span><span style=display:flex><span>        sentence <span style=color:#f92672>=</span> text_to_word_sequence(sentence)
</span></span><span style=display:flex><span>        new_sentence <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        words <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> word <span style=color:#f92672>in</span> sentence:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                word <span style=color:#f92672>=</span> index[word]
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>except</span>:
</span></span><span style=display:flex><span>                <span style=color:#a6e22e>KeyError</span>
</span></span><span style=display:flex><span>                word <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>            words<span style=color:#f92672>.</span>append(word)
</span></span><span style=display:flex><span>        new_sentence<span style=color:#f92672>.</span>append(words)
</span></span><span style=display:flex><span>        new_sentence <span style=color:#f92672>=</span> preprocessing<span style=color:#f92672>.</span>sequence<span style=color:#f92672>.</span>pad_sequences(new_sentence, maxlen<span style=color:#f92672>=</span>maxlen, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;post&#39;</span>)
</span></span><span style=display:flex><span>        new_sentences<span style=color:#f92672>.</span>append(new_sentence[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span>(new_sentences)
</span></span></code></pre></div><p>Next is a function that will index each and every token within the text file and append it to a .json file that will store every token and its associated index number:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_index</span>(texts, filename):
</span></span><span style=display:flex><span>    words <span style=color:#f92672>=</span> texts<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> Tokenizer(num_words <span style=color:#f92672>=</span> <span style=color:#ae81ff>100000</span>)
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>.</span>fit_on_texts(words)
</span></span><span style=display:flex><span>    sequences <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>texts_to_sequences(words)
</span></span><span style=display:flex><span>    word_index <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>word_index
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Found </span><span style=color:#e6db74>{</span>len(word_index)<span style=color:#e6db74>}</span><span style=color:#e6db74> unique words.&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(filename, <span style=color:#e6db74>&#34;w&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        json<span style=color:#f92672>.</span>dump(word_index, f, indent<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
</span></span></code></pre></div><p>In the tutorial, Mattingly uses a max_length of 25 words; a quick graph of the average sentence lengths for each of our author&rsquo;s texts would suggest that number is probably not a bad choice for our dataset:</p><p><img src=/images/imgforblogposts/post_10/jewett_mean_sentence_lengths.png alt="Jewett&amp;rsquo;s Average Sentence Lengths"></p><p><img src=/images/imgforblogposts/post_10/cather_mean_sentence_lengths.png alt="Cather&amp;rsquo;s Average Sentence Lengths"></p><p>We also need to label all of the sentences so we can keep all the sentences and words by Cather paired up together with all the sentences and words by Jewett:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>label_data</span>(sentences, label):
</span></span><span style=display:flex><span>    total_chunks <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> sentence <span style=color:#f92672>in</span> sentences:
</span></span><span style=display:flex><span>        total_chunks<span style=color:#f92672>.</span>append((sentence, label))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span>(total_chunks)
</span></span></code></pre></div><p>Now with all the data structured and ordered in the way the keras model needs it, we can create the training data, create, train, and fit the model on the dataset:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_model</span>(model, tt_data, val_size<span style=color:#f92672>=</span><span style=color:#ae81ff>.3</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>    vals <span style=color:#f92672>=</span> int(len(tt_data[<span style=color:#ae81ff>0</span>])<span style=color:#f92672>*</span>val_size)
</span></span><span style=display:flex><span>    training_data <span style=color:#f92672>=</span> tt_data[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    training_labels <span style=color:#f92672>=</span> tt_data[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    testing_data <span style=color:#f92672>=</span> tt_data[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>    testing_labels <span style=color:#f92672>=</span> tt_data[<span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x_val <span style=color:#f92672>=</span> training_data[:vals]
</span></span><span style=display:flex><span>    x_train <span style=color:#f92672>=</span> training_data[vals:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    y_val <span style=color:#f92672>=</span> training_labels[:vals]
</span></span><span style=display:flex><span>    y_train <span style=color:#f92672>=</span> training_labels[vals:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    fitModel <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(x_train, y_train, epochs<span style=color:#f92672>=</span>epochs, batch_size<span style=color:#f92672>=</span>batch_size, validation_data<span style=color:#f92672>=</span>(x_val, y_val), verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    print(fitModel<span style=color:#f92672>.</span>history<span style=color:#f92672>.</span>keys())
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(fitModel<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;loss&#39;</span>])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(fitModel<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;val_loss&#39;</span>])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;model loss&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;loss&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;epoch&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>legend([<span style=color:#e6db74>&#39;train&#39;</span>, <span style=color:#e6db74>&#39;test&#39;</span>], loc<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;upper left&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>clf()
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(fitModel<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(fitModel<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;val_accuracy&#39;</span>])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Model accuracy&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Accuracy&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epoch&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>legend([<span style=color:#e6db74>&#39;Train&#39;</span>, <span style=color:#e6db74>&#39;Val&#39;</span>], loc<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lower right&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>    model_results <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(testing_data, testing_labels)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span>(model)
</span></span></code></pre></div><p>After testing, the loss and accuracy for the model turned out to be <code>[0.32046514101210954, 0.875969]</code> respectively. The keras library also allows us to see the loss and accuracy over each epoch of training:</p><p><img src=/images/imgforblogposts/post_10/model_loss.png alt="Model Loss"></p><p><img src=/images/imgforblogposts/post_10/model_accuracy.png alt="Model Accuracy"></p><p>In the code snippet above the plot was included in the <code>train_model</code> function, but one could just as easily pull them out as their own functions so we have one function only do one specific job.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_model_loss</span>(model_name, string_1<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;loss&#39;</span>, string_2<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;val_loss&#39;</span>):
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(model_name<span style=color:#f92672>.</span>history[string_1])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(model_name<span style=color:#f92672>.</span>history[string_2])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;model loss&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(string_1)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;epoch&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>legend([<span style=color:#e6db74>&#39;train&#39;</span>, <span style=color:#e6db74>&#39;test&#39;</span>], loc<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;upper left&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_model_accuracy</span>(model_name, string_1<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;accuracy&#39;</span>, string_2<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;val_accuracy&#39;</span>):
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(model_name<span style=color:#f92672>.</span>history[string_1])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(model_name<span style=color:#f92672>.</span>history[string_2])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;model accuracy&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;accuracy&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;epoch&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>legend([<span style=color:#e6db74>&#39;train&#39;</span>, <span style=color:#e6db74>&#39;val&#39;</span>], loc<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lower right&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p>What happens when we turn the model on a text by, say, Cather, that it has not seen yet before? I quite enjoy her <a href=https://cather.unl.edu/writings/shortfiction/ss006>short story</a>, &ldquo;Paul&rsquo;s Case: A Study in Temperament.&rdquo; The script will read in the text, utilize the <code>word_index.json</code> file to replace each word with the number of that word in the index. One can then keep track of each sentence to see the model&rsquo;s predictions: a score close to <code>0</code> suggests the model thinks it&rsquo;s by Cather; the closer to <code>1</code>, the prediction is that it belongs to Jewett. (The full test log <a href=https://github.com/kspicer80/authorship_attribution_studies/blob/main/cather_jewett/logging_predictions/test_log_file.txt>file</a> from the console is available here.) We could easily write <a href=https://github.com/kspicer80/authorship_attribution_studies/blob/main/cather_jewett/interpreting_model_results/model_results_to_df.py>a little bit of code</a> as well to get the .txt file with all the predictions into a DataFrame for even further analysis; it&rsquo;s relatively quick and simple to get some nice scatter plots of the predictions for each and every sentence in the story:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>file <span style=color:#f92672>=</span> <span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;cather_jewett\interpreting_model_results\mate_of_the_daylight_results.txt&#39;</span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(file, header<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, sep<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;,&#39;</span>)
</span></span><span style=display:flex><span>results<span style=color:#f92672>.</span>rename(columns<span style=color:#f92672>=</span>{<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#39;prediction&#39;</span>, <span style=color:#ae81ff>1</span>: <span style=color:#e6db74>&#39;text&#39;</span>}, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>results[<span style=color:#e6db74>&#39;test&#39;</span>] <span style=color:#f92672>=</span> results[<span style=color:#e6db74>&#39;prediction&#39;</span>]<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>extract(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;((?&lt;=\[).*?(?=\]))&#39;</span>)
</span></span><span style=display:flex><span>results[<span style=color:#e6db74>&#39;cleaned_prediction&#39;</span>] <span style=color:#f92672>=</span> results[<span style=color:#e6db74>&#39;test&#39;</span>]<span style=color:#f92672>.</span>astype(float)
</span></span><span style=display:flex><span>cather_df <span style=color:#f92672>=</span> results<span style=color:#f92672>.</span>loc[results<span style=color:#f92672>.</span>cleaned_prediction <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>0.5</span>]
</span></span><span style=display:flex><span>jewett_df <span style=color:#f92672>=</span> results<span style=color:#f92672>.</span>loc[results<span style=color:#f92672>.</span>cleaned_prediction <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0.5</span>]
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> results[<span style=color:#e6db74>&#39;cleaned_prediction&#39;</span>]
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(predictions)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>clf()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(predictions<span style=color:#f92672>.</span>index, predictions<span style=color:#f92672>.</span>values, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.4</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;firebrick&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;sentence&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;prediction&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Predictions for Each Sentence of Cather&#39;s &#39;Paul&#39;s Case&#39;&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img src=/images/imgforblogposts/post_10/model_predictions_scatter_for_pauls_case.png alt="Predictions on Cather&amp;rsquo;s &amp;ldquo;Paul&amp;rsquo;s Case&amp;rdquo;"></p><p>Most of the sentences are predicted to be by Cather. Curiously, feeding the model a text by Jewett, &ldquo;A Mate of the Daylight,&rdquo; most of the predictions are towards the Cather side of the graph:</p><p><img src=/images/imgforblogposts/post_10/model_predictions_scatter_for_mate.png alt="Predictions for &amp;ldquo;Mate&amp;rdquo;"></p><p>It would no doubt be good to do a little bit of digging here to see what might be going on with the model&rsquo;s predictions about this Jewett story. Also, hopefully, in the near future, I can do a little bit of writing about some of the other classifiers one could build and set to work on these Cather and Jewett texts.</p><p>Again, as usual, more to come &mldr;</p></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=https://kspicer80.github.io/tags/machine-learning/>machine learning</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/willa-cather/>willa cather</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/sarah-orne-jewett/>sarah orne jewett</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/digital-humanities/>digital humanities</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/python-for-the-digital-humanities/>python for the digital humanities</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/python/>Python</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/k-nearest-neighbors/>k-nearest neighbors</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/keras/>keras</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/tensorflow/>tensorflow</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/literary-style/>literary style</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/word-frequency/>word frequency</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/word-frequencies/>word frequencies</a></span>
<span class=tag><a href=https://kspicer80.github.io/tags/sklearn/>sklearn</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>1069 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>2022-04-01 00:00</p></div><hr><div class=sharing-buttons><a class=resp-sharing-button__link href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f" target=_blank rel=noopener aria-label title="Share on facebook"><div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 2h-3a5 5 0 00-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 011-1h3z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?url=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f" target=_blank rel=noopener aria-label title="Share on twitter"><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://www.tumblr.com/widgets/share/tool?posttype=link&title=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett&caption=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett&canonicalUrl=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f" target=_blank rel=noopener aria-label title="Share on tumblr"><div class="resp-sharing-button resp-sharing-button--tumblr resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14.563 24c-5.093.0-7.031-3.756-7.031-6.411V9.747H5.116V6.648c3.63-1.313 4.512-4.596 4.71-6.469C9.84.051 9.941.0 9.999.0h3.517v6.114h4.801v3.633h-4.82v7.47c.016 1.001.375 2.371 2.207 2.371h.09c.631-.02 1.486-.205 1.936-.419l1.156 3.425c-.436.636-2.4 1.374-4.156 1.404h-.178l.011.002z"/></svg></div></div></a><a class=resp-sharing-button__link href="mailto:?subject=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett&body=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f" target=_self rel=noopener aria-label title="Share via email"><div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></div></div></a><a class=resp-sharing-button__link href="https://pinterest.com/pin/create/button/?url=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f&media=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f;description=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett" target=_blank rel=noopener aria-label title="Share on pinterest"><div class="resp-sharing-button resp-sharing-button--pinterest resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none"><path d="M12.017.0C5.396.0.029 5.367.029 11.987c0 5.079 3.158 9.417 7.618 11.162-.105-.949-.199-2.403.041-3.439.219-.937 1.406-5.957 1.406-5.957s-.359-.72-.359-1.781c0-1.663.967-2.911 2.168-2.911 1.024.0 1.518.769 1.518 1.688.0 1.029-.653 2.567-.992 3.992-.285 1.193.6 2.165 1.775 2.165 2.128.0 3.768-2.245 3.768-5.487.0-2.861-2.063-4.869-5.008-4.869-3.41.0-5.409 2.562-5.409 5.199.0 1.033.394 2.143.889 2.741.099.12.112.225.085.345-.09.375-.293 1.199-.334 1.363-.053.225-.172.271-.401.165-1.495-.69-2.433-2.878-2.433-4.646.0-3.776 2.748-7.252 7.92-7.252 4.158.0 7.392 2.967 7.392 6.923.0 4.135-2.607 7.462-6.233 7.462-1.214.0-2.354-.629-2.758-1.379l-.749 2.848c-.269 1.045-1.004 2.352-1.498 3.146 1.123.345 2.306.535 3.55.535 6.607.0 11.985-5.365 11.985-11.987C23.97 5.39 18.592.026 11.985.026L12.017.0z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f&title=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett&summary=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett&source=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f" target=_blank rel=noopener aria-label title="Share on linkedin"><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></div></div></a><a class=resp-sharing-button__link href="https://reddit.com/submit/?url=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f&resubmit=true&title=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett" target=_blank rel=noopener aria-label title="Share on reddit"><div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none"><path d="M12 0A12 12 0 000 12a12 12 0 0012 12 12 12 0 0012-12A12 12 0 0012 0zm5.01 4.744c.688.0 1.25.561 1.25 1.249a1.25 1.25.0 01-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968.0 1.754.786 1.754 1.754.0.716-.435 1.333-1.01 1.614a3.111 3.111.0 01.042.52c0 2.694-3.13 4.87-7.004 4.87s-7.004-2.176-7.004-4.87c0-.183.015-.366.043-.534A1.748 1.748.0 014.028 12c0-.968.786-1.754 1.754-1.754.463.0.898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342.0 01.14-.197.35.35.0 01.238-.042l2.906.617a1.214 1.214.0 011.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687.0 1.248-.561 1.248-1.249S9.937 12 9.249 12zm5.5.0c-.687.0-1.248.561-1.248 1.25.0.687.561 1.248 1.249 1.248S16 13.937 16 13.249c0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327.0 00-.231.094.33.33.0 000 .463c.842.842 2.484.913 2.961.913s2.105-.056 2.961-.913a.361.361.0 00.029-.463.33.33.0 00-.464.0c-.547.533-1.684.73-2.512.73-.828.0-1.979-.196-2.512-.73a.326.326.0 00-.232-.095z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://www.xing.com/app/user?op=share;url=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f;title=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett" target=_blank rel=noopener aria-label title="Share on xing"><div class="resp-sharing-button resp-sharing-button--xing resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none"><path d="M18.188.0c-.517.0-.741.325-.927.66.0.0-7.455 13.224-7.702 13.657.015.024 4.919 9.023 4.919 9.023.17.308.436.66.967.66h3.454c.211.0.375-.078.463-.22.089-.151.089-.346-.009-.536l-4.879-8.916c-.004-.006-.004-.016.0-.022L22.139.756c.095-.191.097-.387.006-.535C22.056.078 21.894.0 21.686.0h-3.498zM3.648 4.74c-.211.0-.385.074-.473.216-.09.149-.078.339.02.531l2.34 4.05c.004.01.004.016.0.021L1.86 16.051c-.099.188-.093.381.0.529.085.142.239.234.45.234h3.461c.518.0.766-.348.945-.667l3.734-6.609-2.378-4.155c-.172-.315-.434-.659-.962-.659H3.648v.016z"/></svg></div></div></a><a class=resp-sharing-button__link href="whatsapp://send?text=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett%20https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f" target=_blank rel=noopener aria-label title="Share on whatsapp"><div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198.0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479.0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87.0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86.0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64.0 5.122 1.03 6.988 2.898a9.825 9.825.0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815.0 0012.05.0C5.495.0.16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882.0 005.683 1.448h.005c6.554.0 11.89-5.335 11.893-11.893a11.821 11.821.0 00-3.48-8.413z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f&t=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett" target=_blank rel=noopener aria-label title="Share on hacker news"><div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none"><path d="M0 24V0h24v24H0zM6.951 5.896l4.112 7.708v5.064h1.583v-4.972l4.148-7.799h-1.749l-2.457 4.875c-.372.745-.688 1.434-.688 1.434s-.297-.708-.651-1.434L8.831 5.896h-1.88z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://telegram.me/share/url?text=Binary%20Text%20Classification%20Fun%21%20...%20with%20some%20Literary%20Texts%20by%20Willa%20Cather%20and%20Sarah%20Orne%20Jewett&url=https%3a%2f%2fkspicer80.github.io%2fposts%2f2022-04-10-classification-fun-with-some-literary-texts%2f" target=_blank rel=noopener aria-label title="Share on telegram"><div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="22" y1="2" x2="11" y2="13"/><polygon points="22 2 15 22 11 13 2 9 22 2"/></svg></div></div></a></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button next"><a href=https://kspicer80.github.io/posts/2022-03-29-vector-space-models-and-shakespeare/><span class=button__text>Using Vector Space Models with Shakespeare's Plays</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer></footer></div><script type=text/javascript src=/bundle.min.2d3d449fc0ff117f00ac91342a8f76cd5b710411d7a0254dbe75da3234d2f685d6a0c44cff60c414e90b6a149da8e4032c713c25e4e6838e2e3918dc0ad2e81c.js integrity="sha512-LT1En8D/EX8ArJE0Ko92zVtxBBHXoCVNvnXaMjTS9oXWoMRM/2DEFOkLahSdqOQDLHE8JeTmg44uORjcCtLoHA=="></script></body></html>